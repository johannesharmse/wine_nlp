{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from glove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in dataset 1: 150930\n",
      "Number of entries in dataset 2: 129971\n",
      "\n",
      "Number of duplicate entries across datasets: 48346\n",
      "\n",
      "Number of unique reviews: 169461\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  points\n",
       "0  This tremendous 100% varietal wine hails from ...      96\n",
       "1  Ripe aromas of fig, blackberry and cassis are ...      96\n",
       "2  Mac Watson honors the memory of a wine once ma...      96\n",
       "3  This spent 20 months in 30% new French oak, an...      96\n",
       "4  This is the top wine from La Bégude, named aft...      95"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns to use\n",
    "cols=['description', 'points']\n",
    "\n",
    "# import data\n",
    "reviews_1 = pd.read_csv('../../data/wine-reviews/winemag-data_first150k.csv', index_col=False, usecols=cols)\n",
    "reviews_2 = pd.read_csv('../../data/wine-reviews/winemag-data-130k-v2.csv', index_col=False, usecols=cols)\n",
    "\n",
    "print(\"Number of entries in dataset 1: %s\" %reviews_1.shape[0])\n",
    "print(\"Number of entries in dataset 2: %s\" %reviews_2.shape[0])\n",
    "\n",
    "duplicates = set(reviews_1.description).intersection(set(reviews_2.description))\n",
    "\n",
    "print(\"\\nNumber of duplicate entries across datasets: %s\" % len(duplicates))\n",
    "\n",
    "# concatenate and drop duplicates\n",
    "data = pd.concat([reviews_1,reviews_2])\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"\\nNumber of unique reviews: %s\" % data.shape[0])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_remove = string.punctuation\n",
    "punc_remove = punc_remove.replace('%', '')\n",
    "table = str.maketrans(dict.fromkeys(punc_remove))\n",
    "\n",
    "# lowercase\n",
    "data['description_test'] = data.description_test.str.lower()\n",
    "# remove punctuation\n",
    "data['description_test'] = data.description_test.str.translate(table)\n",
    "# replace percentage sign\n",
    "data['description_test'] = data.description_test.str.replace('%', ' percent')\n",
    "# split words\n",
    "data['description_test'] = data.description_test.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>description_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>96</td>\n",
       "      <td>[this, tremendous, 100, percent, varietal, win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>96</td>\n",
       "      <td>[ripe, aromas, of, fig, blackberry, and, cassi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>96</td>\n",
       "      <td>[mac, watson, honors, the, memory, of, a, wine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>96</td>\n",
       "      <td>[this, spent, 20, months, in, 30, percent, new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>95</td>\n",
       "      <td>[this, is, the, top, wine, from, la, bégude, n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  points  \\\n",
       "0  This tremendous 100% varietal wine hails from ...      96   \n",
       "1  Ripe aromas of fig, blackberry and cassis are ...      96   \n",
       "2  Mac Watson honors the memory of a wine once ma...      96   \n",
       "3  This spent 20 months in 30% new French oak, an...      96   \n",
       "4  This is the top wine from La Bégude, named aft...      95   \n",
       "\n",
       "                                    description_test  \n",
       "0  [this, tremendous, 100, percent, varietal, win...  \n",
       "1  [ripe, aromas, of, fig, blackberry, and, cassi...  \n",
       "2  [mac, watson, honors, the, memory, of, a, wine...  \n",
       "3  [this, spent, 20, months, in, 30, percent, new...  \n",
       "4  [this, is, the, top, wine, from, la, bégude, n...  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 58541\n"
     ]
    }
   ],
   "source": [
    "def create_vocabulary(documents):\n",
    "    \"\"\"Unique words and counts\"\"\"\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for row in documents:\n",
    "        vocabulary.update(row)\n",
    "        \n",
    "    return vocabulary\n",
    "\n",
    "documents = list(data.description_test)\n",
    "vocabulary = create_vocabulary(documents)\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "print(\"Number of unique words: %s\" % vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_vocabulary(vocabulary, n_words=10000):\n",
    "    \"\"\"Limit vocabulary to highest occurring words and create IDs.\"\"\"\n",
    "    vocabulary_n = list(dict(vocabulary.most_common(n_words - 1)).keys())\n",
    "    vocabulary_n.append('UNK') # placeholder for rare words\n",
    "    \n",
    "    vocabulary_n = dict(zip(vocabulary_n, random.sample(range(0, n_words+1), n_words)))\n",
    "            \n",
    "    return vocabulary_n\n",
    "\n",
    "vocabulary_n = top_vocabulary(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_vocabulary(vocabulary, map_table):\n",
    "    \"\"\"Map vocabulary words to IDs\"\"\"\n",
    "    vocabulary_map_table = dict.fromkeys(vocabulary.keys(), 0)\n",
    "    for word in vocabulary:\n",
    "        if word not in map_table:\n",
    "            vocabulary_map_table[word] = map_table['UNK']\n",
    "        else:\n",
    "            vocabulary_map_table[word] = map_table[word]\n",
    "            \n",
    "    return vocabulary_map_table\n",
    "    \n",
    "vocabulary_map_table = map_vocabulary(vocabulary, vocabulary_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_documents(documents, vocabulary_map_table):\n",
    "    \"\"\"Map documents to integer word IDs\"\"\"\n",
    "    documents_mapped = [[vocabulary_map_table[word] for word in doc] for doc in documents]\n",
    "    \n",
    "    return documents_mapped\n",
    "    \n",
    "documents_mapped = map_documents(documents, vocabulary_map_table)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_index = 0 # keep track of training batches\n",
    "word_index = 0\n",
    "total_num_words = sum([len(doc) for doc in documents_mapped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6868514"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(documents_mapped, batch_size, num_skips, skip_window):\n",
    "    \n",
    "    global review_index\n",
    "    global word_index\n",
    "#     global total_num_word\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # span considers window on both sides of target word\n",
    "    # we could potentially consider a full sentence or review\n",
    "    # as the context, but keeping it simpler for now\n",
    "    span = 2 * skip_window + 1 \n",
    "    \n",
    "    # init buffer (context and target words)\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    # go back to first review if no more reviews left\n",
    "    if review_index >= len(documents_mapped):\n",
    "        review_index = 0\n",
    "    \n",
    "    # make sure there are enough words for skip-gram\n",
    "    # could consider moving to next review instead of recycling words\n",
    "    if word_index + span > len(documents_mapped[word_index]):\n",
    "        word_index = len(documents_mapped[word_index]) - span\n",
    "    \n",
    "    # new skip-gram\n",
    "    buffer.extend(documents_mapped[review_index][word_index:word_index + span])\n",
    "        \n",
    "    for i in range(batch_size // num_skips):\n",
    "        # skip_window is the same as index of target word\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        # randomly select context word indices to use\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            # target word\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            # context word\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        \n",
    "        # add next words / new review for skips\n",
    "        if word_index + span == len(documents_mapped[word_index]):\n",
    "            # next review\n",
    "            word_index = 0\n",
    "            review_index += 1\n",
    "            # start at the beginning if non left\n",
    "            if review_index >= len(documents_mapped):\n",
    "                review_index = 0\n",
    "            \n",
    "            buffer.extend(documents_mapped[review_index][word_index:word_index + span])\n",
    "            \n",
    "            word_index += span\n",
    "        else:\n",
    "            # add next word in review\n",
    "            buffer.append(documents_mapped[review_index][word_index])\n",
    "            word_index += 1\n",
    "            \n",
    "    return batch, labels\n",
    "            \n",
    "batch_size = 50\n",
    "num_skips = 5\n",
    "skip_window = 3\n",
    "test_batch, test_labels = generate_batch(documents_mapped, batch_size, num_skips, skip_window)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_batch.shape[0] == batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sample:\n",
      "[1654 1654 1654 1654 1654 5564 5564 5564 5564 5564]\n",
      "\n",
      "Context Sample:\n",
      "[[2843]\n",
      " [6133]\n",
      " [6621]\n",
      " [9040]\n",
      " [5564]\n",
      " [1654]\n",
      " [8424]\n",
      " [6621]\n",
      " [6133]\n",
      " [2843]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target sample:\")\n",
    "print(test_batch[:10])\n",
    "print(\"\\nContext Sample:\")\n",
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "\n",
    "batch_size = 50\n",
    "num_skips = 5\n",
    "skip_window = 3\n",
    "\n",
    "num_sampled = 32 # number of negative examples to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # input data\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=batch_size) # target\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) # context (what we want to predict)\n",
    "        \n",
    "    with tf.name_scope('embeddings'):\n",
    "        # embedding weights\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    # hidden layer weights\n",
    "    with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    # hidden layers biases\n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "    # NCE because softmax is too expensive\n",
    "    # is it a good or a corrupt pair (context and target)?\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights, \n",
    "                biases=nce_biases, \n",
    "                labels=train_labels, \n",
    "                inputs=embed, \n",
    "                num_sampled=num_sampled, \n",
    "                num_classes=vocabulary_size))\n",
    "        \n",
    "    # for viz\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    # SGD - minimize loss on train data (see loss above)\n",
    "    # Learning Rate = 1.0 (don't care about overfitting)\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "#     norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "#     normalized_embeddings = embeddings / norm\n",
    "#     valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid)\n",
    "    \n",
    "    # init variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # saver\n",
    "    saver = tf.train.Saver()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings\n",
    "\n",
    "# latent features\n",
    "embedding_size = 50\n",
    "vocabulary_size = len(vocabulary)\n",
    "# init values\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# initialize weights and biases for word2vect model\n",
    "# each unique word gets a weight per latent feature and a single bias\n",
    "nce_weights = tf.Variable(\n",
    "  tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                      stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for inputs (reviews are read in in batches during training)\n",
    "batch_size = 25\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size // num_skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2941, 2941, 2941, 2941, 2941, 2941, 2941, 2941, 2941, 2941],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch[0][test_batch[0] == 2941]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
